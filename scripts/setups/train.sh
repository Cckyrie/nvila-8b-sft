# #!/bin/bash

# DEFAULT_GPUS_PER_NODE=8
# DEFAULT_MASTER_ADDR="127.0.0.1"
# DEFAULT_MASTER_PORT=25001

# echo "SLURM_JOB_ID = $SLURM_JOB_ID"
# echo "SLURM_JOB_NAME = $SLURM_JOB_NAME"

# RUN_NAME=${RUN_NAME:-$DEFAULT_RUN_NAME}
# RUN_NAME=${RUN_NAME:-$SLURM_JOB_NAME}
# echo "RUN_NAME = $RUN_NAME"

# OUTPUT_DIR=${OUTPUT_DIR:-"runs/train/$RUN_NAME"}
# echo "OUTPUT_DIR = $OUTPUT_DIR"

# export WANDB_PROJECT="vila"
# export WANDB_DIR=$OUTPUT_DIR
# export WANDB_RUN_ID=$RUN_NAME
# export WANDB_NAME=$RUN_NAME
# export WANDB_RESUME="allow"

# NNODES=${SLURM_JOB_NUM_NODES:-1}
# echo "NNODES = $NNODES"

# NODES=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | tr '\n' ' ')
# echo "NODES = $NODES"

# NODE_RANK=${SLURM_PROCID:-0}
# echo "NODE_RANK = $NODE_RANK"

# GPUS_PER_NODE=${SLURM_JOB_GPUS_PER_NODE:-$DEFAULT_GPUS_PER_NODE}
# echo "GPUS_PER_NODE = $GPUS_PER_NODE"

# MASTER_ADDR=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
# MASTER_ADDR=${MASTER_ADDR:-$DEFAULT_MASTER_ADDR}
# echo "MASTER_ADDR = $MASTER_ADDR"

# MASTER_PORT=${MASTER_PORT:-$DEFAULT_MASTER_PORT}
# echo "MASTER_PORT = $MASTER_PORT"

# GLOBAL_TRAIN_BATCH_SIZE=${GLOBAL_TRAIN_BATCH_SIZE:-$DEFAULT_GLOBAL_TRAIN_BATCH_SIZE}
# echo "GLOBAL_TRAIN_BATCH_SIZE = $GLOBAL_TRAIN_BATCH_SIZE"

# GRADIENT_ACCUMULATION_STEPS=${GRADIENT_ACCUMULATION_STEPS:-$DEFAULT_GRADIENT_ACCUMULATION_STEPS}
# echo "GRADIENT_ACCUMULATION_STEPS = $GRADIENT_ACCUMULATION_STEPS"

# PER_DEVICE_TRAIN_BATCH_SIZE=$((GLOBAL_TRAIN_BATCH_SIZE / NNODES / GPUS_PER_NODE / GRADIENT_ACCUMULATION_STEPS))
# echo "PER_DEVICE_TRAIN_BATCH_SIZE = $PER_DEVICE_TRAIN_BATCH_SIZE"

# if [ -n "$MAX_PER_DEVICE_TRAIN_BATCH_SIZE" ] && [ "$PER_DEVICE_TRAIN_BATCH_SIZE" -gt "$MAX_PER_DEVICE_TRAIN_BATCH_SIZE" ]; then
#     echo "PER_DEVICE_TRAIN_BATCH_SIZE is greater than MAX_PER_DEVICE_TRAIN_BATCH_SIZE"
#     exit 1
# fi

# export NCCL_IB_SL=1
# export CUDA_DEVICE_MAX_CONNECTIONS=1
# export TORCH_NCCL_ASYNC_ERROR_HANDLING=1
# export OMP_NUM_THREADS=1

#!/bin/bash

# 默认参数
DEFAULT_GPUS_PER_NODE=4
DEFAULT_MASTER_ADDR="127.0.0.1"
DEFAULT_MASTER_PORT=25001
DEFAULT_NNODES=1
DEFAULT_NODE_RANK=0

# 日志函数
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
}

# 检查 SLURM 环境
SLURM_AVAILABLE=0
if command -v scontrol >/dev/null 2>&1; then
    SLURM_AVAILABLE=1
    log "SLURM environment detected"
else
    log "SLURM not detected, using default values"
fi

# 设置 SLURM 相关变量
log "SLURM_JOB_ID = $SLURM_JOB_ID"
log "SLURM_JOB_NAME = $SLURM_JOB_NAME"

RUN_NAME=${RUN_NAME:-$DEFAULT_RUN_NAME}
RUN_NAME=${RUN_NAME:-$SLURM_JOB_NAME}
RUN_NAME=${RUN_NAME:-"default-run-$(date +%Y%m%d-%H%M%S)"}
log "RUN_NAME = $RUN_NAME"

OUTPUT_DIR=${OUTPUT_DIR:-"runs/train/$RUN_NAME"}
log "OUTPUT_DIR = $OUTPUT_DIR"

# 设置 WandB 配置
export WANDB_PROJECT="vila"
export WANDB_DIR=$OUTPUT_DIR
export WANDB_RUN_ID=$RUN_NAME
export WANDB_NAME=$RUN_NAME
export WANDB_RESUME="allow"

# 设置分布式训练参数
NNODES=${SLURM_JOB_NUM_NODES:-$DEFAULT_NNODES}
log "NNODES = $NNODES"

if [ $SLURM_AVAILABLE -eq 1 ] && [ -n "$SLURM_JOB_NODELIST" ]; then
    NODES=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | tr '\n' ' ')
    MASTER_ADDR=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
else
    NODES="localhost"
    MASTER_ADDR=$DEFAULT_MASTER_ADDR
fi
log "NODES = $NODES"
log "MASTER_ADDR = $MASTER_ADDR"

NODE_RANK=${SLURM_PROCID:-$DEFAULT_NODE_RANK}
log "NODE_RANK = $NODE_RANK"

GPUS_PER_NODE=${SLURM_JOB_GPUS_PER_NODE:-$DEFAULT_GPUS_PER_NODE}
log "GPUS_PER_NODE = $GPUS_PER_NODE"

MASTER_PORT=${MASTER_PORT:-$DEFAULT_MASTER_PORT}
log "MASTER_PORT = $MASTER_PORT"

# 设置批大小参数
GLOBAL_TRAIN_BATCH_SIZE=${GLOBAL_TRAIN_BATCH_SIZE:-$DEFAULT_GLOBAL_TRAIN_BATCH_SIZE}
log "GLOBAL_TRAIN_BATCH_SIZE = $GLOBAL_TRAIN_BATCH_SIZE"

GRADIENT_ACCUMULATION_STEPS=${GRADIENT_ACCUMULATION_STEPS:-$DEFAULT_GRADIENT_ACCUMULATION_STEPS}
log "GRADIENT_ACCUMULATION_STEPS = $GRADIENT_ACCUMULATION_STEPS"

# 计算每设备批大小
if [ $NNODES -eq 0 ] || [ $GPUS_PER_NODE -eq 0 ] || [ $GRADIENT_ACCUMULATION_STEPS -eq 0 ]; then
    log "ERROR: Invalid NNODES, GPUS_PER_NODE, or GRADIENT_ACCUMULATION_STEPS"
    exit 1
fi

PER_DEVICE_TRAIN_BATCH_SIZE=$((GLOBAL_TRAIN_BATCH_SIZE / NNODES / GPUS_PER_NODE / GRADIENT_ACCUMULATION_STEPS))
log "PER_DEVICE_TRAIN_BATCH_SIZE = $PER_DEVICE_TRAIN_BATCH_SIZE"

# 检查最大批大小限制
if [ -n "$MAX_PER_DEVICE_TRAIN_BATCH_SIZE" ] && [ "$PER_DEVICE_TRAIN_BATCH_SIZE" -gt "$MAX_PER_DEVICE_TRAIN_BATCH_SIZE" ]; then
    log "ERROR: PER_DEVICE_TRAIN_BATCH_SIZE ($PER_DEVICE_TRAIN_BATCH_SIZE) exceeds MAX_PER_DEVICE_TRAIN_BATCH_SIZE ($MAX_PER_DEVICE_TRAIN_BATCH_SIZE)"
    exit 1
fi

# 设置 PyTorch 和 NCCL 环境变量
export NCCL_IB_SL=1
export CUDA_DEVICE_MAX_CONNECTIONS=1
export TORCH_NCCL_ASYNC_ERROR_HANDLING=1
export OMP_NUM_THREADS=1
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# 清理 GPU 内存
log "Clearing GPU memory cache"
python -c "import torch; torch.cuda.empty_cache()"

# 检查 GPU 可用性
log "Checking GPU availability"
nvidia-smi || { log "ERROR: nvidia-smi failed, please check GPU setup"; exit 1; }

# 导出环境变量以供后续脚本使用
export NNODES
export NODE_RANK
export GPUS_PER_NODE
export MASTER_ADDR
export MASTER_PORT
export PER_DEVICE_TRAIN_BATCH_SIZE
export OUTPUT_DIR